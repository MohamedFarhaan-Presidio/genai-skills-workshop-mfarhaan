{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge 2 RAG"
      ],
      "metadata": {
        "id": "dc6QVOv4MkMV"
      },
      "id": "dc6QVOv4MkMV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary dependecies"
      ],
      "metadata": {
        "id": "kS9QqtDDMogG"
      },
      "id": "kS9QqtDDMogG"
    },
    {
      "cell_type": "code",
      "id": "9leu8a6IQEleSoN9nZRHPL0W",
      "metadata": {
        "tags": [],
        "id": "9leu8a6IQEleSoN9nZRHPL0W"
      },
      "source": [
        "!pip install google-cloud-bigquery google-cloud-aiplatform pandas --quiet"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticate GCloud SDK"
      ],
      "metadata": {
        "id": "s790PKNJMq90"
      },
      "id": "s790PKNJMq90"
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "Kpe7ECYqGSv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6800dfba-3db6-4fc2-aee9-79b67517b930"
      },
      "id": "Kpe7ECYqGSv3",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are running on a Google Compute Engine virtual machine.\n",
            "The service credentials associated with this virtual machine\n",
            "will automatically be used by Application Default\n",
            "Credentials, so it is not necessary to use this command.\n",
            "\n",
            "If you decide to proceed anyway, your user credentials may be visible\n",
            "to others with access to this virtual machine. Are you sure you want\n",
            "to authenticate with your personal account?\n",
            "\n",
            "Do you want to continue (Y/n)?  y\n",
            "\n",
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=GbHXr6Zp7yq2Usommx67SsnOIe5oSm&prompt=consent&token_usage=remote&access_type=offline&code_challenge=1lT5ZP9Eb-YdZZN489R6prLdpP3JaGO_8BqNqdqQXIY&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AUJR-x61Z8VXaVD5zkY-WawVh-ENA4LdxfV76DqvAH_2fOiTo42eSMO4Jp0wACpRlOSEEA\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"qwiklabs-gcp-03-f951e059a60d\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary packages"
      ],
      "metadata": {
        "id": "qO-XMpPqMx3_"
      },
      "id": "qO-XMpPqMx3_"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from vertexai.preview.language_models import ChatModel\n",
        "from vertexai.preview.generative_models import GenerativeModel\n",
        "import vertexai\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "JUgo_TOYERgP"
      },
      "id": "JUgo_TOYERgP",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize constants"
      ],
      "metadata": {
        "id": "k9l6bkCVM7W8"
      },
      "id": "k9l6bkCVM7W8"
    },
    {
      "cell_type": "code",
      "source": [
        "project_id=\"qwiklabs-gcp-03-f951e059a60d\"\n",
        "dataset_id=\"mfarhaan_challenge_2_dataset\"\n",
        "embedding_conn_name=\"embedding_conn\"\n",
        "embedding_model_name=\"mfarhaan_challenge_2_embedding_model_id\"\n",
        "embedding_model_id=f\"{project_id}.{dataset_id}.{embedding_model_name}\"\n",
        "dataset_table_name=\"mfarhaan_challenge_2_dataset_table\"\n",
        "dataset_table_id=f\"{project_id}.{dataset_id}.{dataset_table_name}\"\n",
        "embedded_dataset_table_name=\"mfarhaan_challenge_2_dataset_embedded_table\"\n",
        "dataset_embedded_table_id=f\"{project_id}.{dataset_id}.{embedded_dataset_table_name}\"\n",
        "vertexai_location=\"us-central1\"\n",
        "bigquery_location=\"us\"\n",
        "chatmodel_location=\"global\"\n",
        "chatmodel_name=\"gemini-2.0-flash-001\"\n",
        "dataset_src=\"gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv\""
      ],
      "metadata": {
        "id": "zPTP1jzEEWCy"
      },
      "id": "zPTP1jzEEWCy",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Vertex AI SDK"
      ],
      "metadata": {
        "id": "EwI0rtnSM9w8"
      },
      "id": "EwI0rtnSM9w8"
    },
    {
      "cell_type": "code",
      "source": [
        "vertexai.init(project=project_id, location=vertexai_location)"
      ],
      "metadata": {
        "id": "aGqCFqMGEeO4"
      },
      "id": "aGqCFqMGEeO4",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Big Query Client and load the CSV data into big query table"
      ],
      "metadata": {
        "id": "eR3dCHW7M_xD"
      },
      "id": "eR3dCHW7M_xD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Big query Load CSV data job configuration\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        "    autodetect=True,\n",
        "    write_disposition=\"WRITE_TRUNCATE\"\n",
        ")\n",
        "\n",
        "# Initializing Big Query client\n",
        "bq = bigquery.Client(project=project_id, location=bigquery_location)\n",
        "\n",
        "#Load CSV data to big query table\n",
        "load_job = bq.load_table_from_uri(dataset_src, dataset_table_id, job_config=job_config)\n",
        "load_job.result()\n",
        "print(\"CSV Data loaded to Big Query table\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxHdaVNbEipW",
        "outputId": "f6f0aaa6-971f-4aca-b1b8-d49b7db9c260"
      },
      "id": "vxHdaVNbEipW",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV Data loaded to Big Query table\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an remote embedding model that can be used in future sql queries to vector search the data"
      ],
      "metadata": {
        "id": "6ZYw_INgJSZ9"
      },
      "id": "6ZYw_INgJSZ9"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an embedding model\n",
        "embed_model = f\"\"\"\n",
        "  CREATE OR REPLACE MODEL `{dataset_id}.{embedding_model_name}`\n",
        "  REMOTE WITH CONNECTION `{bigquery_location}.{embedding_conn_name}`\n",
        "  OPTIONS (ENDPOINT = 'text-embedding-005')\n",
        "\"\"\"\n",
        "bq.query(embed_model).result()\n",
        "print(\"Embedding model created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7TR2YMGL028",
        "outputId": "62a4f67b-e27d-4024-aaa3-444ded9678cb"
      },
      "id": "h7TR2YMGL028",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new embedded data table in big query with source table as the csv loaded table. Use remote embedding connection to embed the data"
      ],
      "metadata": {
        "id": "eSSAwF6vNLsK"
      },
      "id": "eSSAwF6vNLsK"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an embedded table with embedded data\n",
        "embed_table_query = f\"\"\"\n",
        "  CREATE OR REPLACE TABLE `{dataset_id}.{embedded_dataset_table_name}` AS\n",
        "  SELECT *, ml_generate_embedding_result AS vector_embedding\n",
        "  FROM ML.GENERATE_EMBEDDING(\n",
        "    MODEL `{dataset_id}.{embedding_model_name}`,\n",
        "    (\n",
        "      SELECT CONCAT(string_field_0, ' ', string_field_1) AS content,\n",
        "            string_field_0 AS question,\n",
        "            string_field_1 AS answer\n",
        "      FROM `{dataset_id}.{dataset_table_name}`\n",
        "    )\n",
        "  )\n",
        "\"\"\"\n",
        "bq.query(embed_table_query).result()\n",
        "print(\"Embedded table populated\")"
      ],
      "metadata": {
        "id": "cxiur1CYFQoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2496f474-ef92-4b29-ccb1-e543ead9237a"
      },
      "id": "cxiur1CYFQoW",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded table populated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to query the data in vector format in big query embedded table return top \"K\" results as requested"
      ],
      "metadata": {
        "id": "WswstpgqNVjO"
      },
      "id": "WswstpgqNVjO"
    },
    {
      "cell_type": "code",
      "source": [
        "# This function queries the embedded data table. It accepts user's query and retireves top \"k\" matching results\n",
        "def query_embdded_table(query, top_k = 3):\n",
        "    query = f\"\"\"\n",
        "      SELECT query.query, r.base.question, r.base.answer, r.distance\n",
        "      FROM VECTOR_SEARCH(\n",
        "        TABLE `{dataset_id}.{embedded_dataset_table_name}`,\n",
        "        'vector_embedding',\n",
        "        (\n",
        "          SELECT\n",
        "            ml_generate_embedding_result AS vector_embedding,\n",
        "            '{query}' AS query\n",
        "          FROM ML.GENERATE_EMBEDDING(\n",
        "            MODEL `{dataset_id}.{embedding_model_name}`,\n",
        "            (SELECT '{query}' AS content)\n",
        "          )\n",
        "        ),\n",
        "        top_k => {top_k},\n",
        "        options => '{{\"fraction_lists_to_search\": 1.0}}'\n",
        "      ) AS r\n",
        "    \"\"\"\n",
        "    return bq.query(query).to_dataframe()"
      ],
      "metadata": {
        "id": "a6IDRGDgFrmI"
      },
      "id": "a6IDRGDgFrmI",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a chat model to retireve response for user's request"
      ],
      "metadata": {
        "id": "_OwFRlW-Njbv"
      },
      "id": "_OwFRlW-Njbv"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a chat model\n",
        "chat_model = GenerativeModel(chatmodel_name)\n",
        "\n",
        "#Create a chat function which accepts user's query\n",
        "def chat(query):\n",
        "    print(f\"USER: {query}\")\n",
        "    results = query_embdded_table(query)\n",
        "    context = \"\\n\\n\".join([f\"Q: {row['question']}\\nA: {row['answer']}\" for _, row in results.iterrows()])\n",
        "    prompt = f\"\"\"\n",
        "      Answer to the question based on the context. DO not answer on your own.\n",
        "\n",
        "      Context:\n",
        "        {context}\n",
        "\n",
        "      Question: {query}\n",
        "    :\\n\\n{context}\\n\\nUser: {query}\"\"\"\n",
        "    results = chat_model.generate_content(prompt)\n",
        "    print(f\"SYSTEM: {results.text}\")"
      ],
      "metadata": {
        "id": "Guvn9_CkGIbA"
      },
      "id": "Guvn9_CkGIbA",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intiate RAG questions"
      ],
      "metadata": {
        "id": "AdRziXeNPU8y"
      },
      "id": "AdRziXeNPU8y"
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"When was Aurora bay founded\")"
      ],
      "metadata": {
        "id": "zSicqnWxGOKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3916d612-e0e1-4ada-aaec-751371d8a736"
      },
      "id": "zSicqnWxGOKf",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER: When was Aurora bay founded\n",
            "SYSTEM: Aurora Bay was founded in 1901 by a group of fur traders who recognized the regionâ€™s strategic coastal location.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "chl2_2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}